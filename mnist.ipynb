{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from imageio import imread\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import cytoolz as cz\n",
    "import tensorflow as tf\n",
    "import idx2numpy\n",
    "import functools\n",
    "from dataget import data \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data(\"mnist\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab8dfe21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  60000\n"
     ]
    }
   ],
   "source": [
    "df = dataset.training_set.dataframe()\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "df.class_id.hist(bins=10, color='r')\n",
    "plt.xlabel('Class Id')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "print('Dataset size: ', len(df.class_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for g, f in df.groupby(np.arange(len(df)) // 120):\n",
    "# #     print(type(f))\n",
    "\n",
    "# # def get_input_data(dataframe, batch_size, epochs, shuffle):\n",
    "# #     if epochs is not None:\n",
    "\n",
    "# def pp(data_set, epochs, steps, batch_size):\n",
    "    \n",
    "#     if epochs is not None and epochs >= 1:\n",
    "#         for _ in range(epochs):\n",
    "#             for _, f in data_set.groupby(np.arange(len(data_set)) // batch_size):\n",
    "#                 yield f\n",
    "                \n",
    "#     elif epochs is None:\n",
    "#         for _ in range(steps):\n",
    "#             data_set = data_set.sample(n=batch_size)\n",
    "#             image = np.array([i for i in data_set.image])\n",
    "#             labels = data_set['class_id'].values\n",
    "#             yield labels.shape\n",
    "\n",
    "# for j, i in enumerate(pp(df, epochs=2, steps=0, batch_size=5)):\n",
    "#     if j < 10:\n",
    "#         print(i.filename.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44709</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/ubuntu/.dataget/data/mnist/training-set/...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22189</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/ubuntu/.dataget/data/mnist/training-set/...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/ubuntu/.dataget/data/mnist/training-set/...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55887</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/ubuntu/.dataget/data/mnist/training-set/...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56292</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/ubuntu/.dataget/data/mnist/training-set/...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class_id                                           filename  \\\n",
       "44709         4  /home/ubuntu/.dataget/data/mnist/training-set/...   \n",
       "22189         5  /home/ubuntu/.dataget/data/mnist/training-set/...   \n",
       "2686          0  /home/ubuntu/.dataget/data/mnist/training-set/...   \n",
       "55887         2  /home/ubuntu/.dataget/data/mnist/training-set/...   \n",
       "56292         2  /home/ubuntu/.dataget/data/mnist/training-set/...   \n",
       "\n",
       "                                                   image  \n",
       "44709  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "22189  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2686   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,...  \n",
       "55887  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "56292  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "\n",
    "# Network Parameters\n",
    "num_classes = 10 \n",
    "dropout = 0.25 # Dropout, probability to dnput_fn = tf.estimator.inputs.numpy_inpurop a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def conv_net(features, labels, mode):\n",
    "    \n",
    "    print()\n",
    "    print(\"#######################\")\n",
    "    print(\"Model\")\n",
    "    print(\"#######################\")\n",
    "    \n",
    "    input_layer = tf.reshape(features[\"image\"], [-1, 28, 28, 1]);print(input_layer)\n",
    "\n",
    "    conv1 = tf.layers.conv2d(input_layer, 32, 5, activation=tf.nn.relu); print(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(conv1, 2, 2); print(pool1)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(pool1, 64, 5, activation=tf.nn.relu); print(conv2)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, 2, 2); print(pool2)\n",
    "\n",
    "    # Flatten the data to a 1-D vector for the fully connected layer\n",
    "    pool2_flat = tf.contrib.layers.flatten(pool2); print(pool2_flat)\n",
    "\n",
    "    #fully connected layer\n",
    "    dense = tf.layers.dense(pool2_flat, 1024, activation=tf.nn.relu); print(dense)\n",
    "\n",
    "    #dropout\n",
    "    fc1 = tf.layers.dropout(dense, rate=dropout, training=mode == tf.estimator.ModeKeys.TRAIN); print(fc1)\n",
    "    logits = tf.layers.dense(fc1, units=num_classes); print(logits)\n",
    "    print(\"#######################\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=num_classes)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fab8dfe2e80>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=conv_net, \n",
    "        model_dir=\"/tmp/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, epochs=epochs, steps=num_steps, batch_size=batch_size):\n",
    "    \n",
    "    if epochs:\n",
    "        steps = int(len(df)/batch_size)*epochs\n",
    "        \n",
    "    for _ in range(steps):\n",
    "        data_set = df.sample(n=batch_size)\n",
    "        image = np.array([i for i in data_set.image])\n",
    "        labels = data_set['class_id'].values\n",
    "        yield image, labels\n",
    "          \n",
    "def input_fn(df):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        functools.partial(process, df=df),\n",
    "        (tf.float32, tf.float32),\n",
    "        (tf.TensorShape([batch_size, 28, 28]), tf.TensorShape([batch_size]))\n",
    "    )\n",
    "    features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    return {'image': features}, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################\n",
      "Model\n",
      "#######################\n",
      "Tensor(\"Reshape:0\", shape=(100, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"conv2d/Relu:0\", shape=(100, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"max_pooling2d/MaxPool:0\", shape=(100, 12, 12, 32), dtype=float32)\n",
      "Tensor(\"conv2d_2/Relu:0\", shape=(100, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(100, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"Flatten/flatten/Reshape:0\", shape=(100, 1024), dtype=float32)\n",
      "Tensor(\"dense/Relu:0\", shape=(100, 1024), dtype=float32)\n",
      "Tensor(\"dropout/dropout/mul:0\", shape=(100, 1024), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(100, 10), dtype=float32)\n",
      "#######################\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/mnist/model.ckpt.\n",
      "INFO:tensorflow:loss = 36.1729, step = 1\n",
      "INFO:tensorflow:global_step/sec: 82.8261\n",
      "INFO:tensorflow:loss = 0.321721, step = 101 (1.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.4974\n",
      "INFO:tensorflow:loss = 0.274213, step = 201 (1.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.6084\n",
      "INFO:tensorflow:loss = 0.0847785, step = 301 (1.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7145\n",
      "INFO:tensorflow:loss = 0.0475526, step = 401 (1.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.7752\n",
      "INFO:tensorflow:loss = 0.0177648, step = 501 (1.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.1872\n",
      "INFO:tensorflow:loss = 0.271229, step = 601 (1.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.4772\n",
      "INFO:tensorflow:loss = 0.325348, step = 701 (1.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.453\n",
      "INFO:tensorflow:loss = 0.0626728, step = 801 (1.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 96.4996\n",
      "INFO:tensorflow:loss = 0.0170455, step = 901 (1.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.2118\n",
      "INFO:tensorflow:loss = 0.016229, step = 1001 (1.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.068\n",
      "INFO:tensorflow:loss = 0.0621586, step = 1101 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.6066\n",
      "INFO:tensorflow:loss = 0.0669884, step = 1201 (1.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0351\n",
      "INFO:tensorflow:loss = 0.0777489, step = 1301 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.5071\n",
      "INFO:tensorflow:loss = 0.0550879, step = 1401 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.3478\n",
      "INFO:tensorflow:loss = 0.0561888, step = 1501 (1.015 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.781\n",
      "INFO:tensorflow:loss = 0.0190567, step = 1601 (1.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.3766\n",
      "INFO:tensorflow:loss = 0.104971, step = 1701 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.6383\n",
      "INFO:tensorflow:loss = 0.0417603, step = 1801 (1.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.0605\n",
      "INFO:tensorflow:loss = 0.0354396, step = 1901 (1.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0646\n",
      "INFO:tensorflow:loss = 0.00361359, step = 2001 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0558\n",
      "INFO:tensorflow:loss = 0.0463464, step = 2101 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1922\n",
      "INFO:tensorflow:loss = 0.0481572, step = 2201 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.9474\n",
      "INFO:tensorflow:loss = 0.00874004, step = 2301 (1.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.8775\n",
      "INFO:tensorflow:loss = 0.0173098, step = 2401 (1.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.8457\n",
      "INFO:tensorflow:loss = 0.0150503, step = 2501 (1.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.3292\n",
      "INFO:tensorflow:loss = 0.0278727, step = 2601 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.5084\n",
      "INFO:tensorflow:loss = 0.0207428, step = 2701 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.4496\n",
      "INFO:tensorflow:loss = 0.127134, step = 2801 (1.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.7032\n",
      "INFO:tensorflow:loss = 0.0642283, step = 2901 (1.023 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /tmp/mnist/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0246672.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fab8dfe2b70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.train(input_fn=functools.partial(input_fn, df=df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################\n",
      "Model\n",
      "#######################\n",
      "Tensor(\"Reshape:0\", shape=(100, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"conv2d/Relu:0\", shape=(100, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"max_pooling2d/MaxPool:0\", shape=(100, 12, 12, 32), dtype=float32)\n",
      "Tensor(\"conv2d_2/Relu:0\", shape=(100, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(100, 4, 4, 64), dtype=float32)\n",
      "Tensor(\"Flatten/flatten/Reshape:0\", shape=(100, 1024), dtype=float32)\n",
      "Tensor(\"dense/Relu:0\", shape=(100, 1024), dtype=float32)\n",
      "Tensor(\"dropout/Identity:0\", shape=(100, 1024), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(100, 10), dtype=float32)\n",
      "#######################\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-20-04:07:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist/model.ckpt-3000\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-20-04:07:18\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.986, global_step = 3000, loss = 0.0503163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.986, 'global_step': 3000, 'loss': 0.050316252}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "model.evaluate(input_fn=functools.partial(input_fn, df=dataset.test_set.dataframe()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
